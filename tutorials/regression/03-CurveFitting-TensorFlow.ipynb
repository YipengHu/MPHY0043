{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u6-He3cXBKfM"
   },
   "source": [
    "# 3 Linear Regression - Curve Fitting (TensorFlow)\n",
    "Two methods are used to fit a curve in this tutorial, using [TensorFlow](https://www.tensorflow.org/):\n",
    "- Direct solution using least-squares method - this is the same method used in the previous tutorial that uses NumPy \n",
    "- Iterative optimisation using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pU5v5NAxBKfQ"
   },
   "source": [
    "## 3.1 Data\n",
    "First, we sample $n$ observed data from the underlying polynomial defined by weights $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2T4JW1SBKfU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# get ground-truth data from the \"true\" model \n",
    "n = 100 \n",
    "w = [4, 3, 2, 1]\n",
    "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
    "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
    "                       np.linspace(len(w)-1,0,len(w))), w)\n",
    "std_noise = 0.2\n",
    "t_observed = np.reshape(\n",
    "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
    "    [-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmgdFSrvBKfc"
   },
   "source": [
    "## 3.2 Computation Graph and Session\n",
    "[Graphs and sessions](https://www.tensorflow.org/guide/graphs) are important features of TensorFlow. In most simple terms, a graph needs to be built to specify what computations are; then sessions are constructed to specify what computation to run, for example, what data to use and in what order. To facilitate the data feeding, [*placeholders*](https://www.tensorflow.org/api_docs/python/tf/placeholder) are used. The following two methods to fit the model provide two examples how these are used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build a computation graph using \"tf functions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1540321798182,
     "user": {
      "displayName": "Yipeng Hu",
      "photoUrl": "",
      "userId": "18139436242730223489"
     },
     "user_tz": -60
    },
    "id": "8w3xBytsBKff",
    "outputId": "23a13225-f139-47dd-8971-18fd75b203ad"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# placeholders are for feeding data in runtime\n",
    "ph_x = tf.placeholder(tf.float32, [n, 1])\n",
    "ph_t = tf.placeholder(tf.float32, [n, 1])\n",
    "\n",
    "deg = 3\n",
    "node_X = tf.pow(ph_x, tf.linspace(tf.to_float(deg),0,deg+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above is a very simple computation graph to evaluate the polynomial using TensorFlow functions. This can be built without any real data and there has not been any computation taking place either.  \n",
    "\n",
    "Then we construct a session. And, call the run method to evaluate the node *node_X* to actually run the computation and obtain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1540321798182,
     "user": {
      "displayName": "Yipeng Hu",
      "photoUrl": "",
      "userId": "18139436242730223489"
     },
     "user_tz": -60
    },
    "id": "8w3xBytsBKff",
    "outputId": "23a13225-f139-47dd-8971-18fd75b203ad"
   },
   "outputs": [],
   "source": [
    "# build a session\n",
    "sess = tf.Session()  \n",
    "\n",
    "# set an example data feed\n",
    "dataFeed = {ph_x:x} \n",
    "\n",
    "# run the session to evaluate the node weights\n",
    "X = sess.run(node_X, feed_dict=dataFeed)\n",
    "print(X[:n:10,])\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUHDEl-yBKft"
   },
   "source": [
    "## 3.3 Least-Squares Solution\n",
    "This is mathematcally the same method used in previous NumPy tutorial. The advantage using TensorFlow here is not particularly obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1540321798797,
     "user": {
      "displayName": "Yipeng Hu",
      "photoUrl": "",
      "userId": "18139436242730223489"
     },
     "user_tz": -60
    },
    "id": "DYZu3MyaBKfw",
    "outputId": "3c7e3462-a1be-4d44-ca7c-70fc54d692b2"
   },
   "outputs": [],
   "source": [
    "# completing the computation graph with the least-square solution\n",
    "node_w = tf.matrix_solve_ls(node_X, ph_t)\n",
    "\n",
    "# run the session to evaluate the node weights\n",
    "sess = tf.Session()  \n",
    "dataFeed = {ph_x:x, ph_t:t_observed}  # feed data\n",
    "w_lstsq = sess.run(node_w, feed_dict=dataFeed)\n",
    "print(w_lstsq)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhy2ym8FBKf8"
   },
   "source": [
    "## 3.3 Stochastic Gradient Descend Method\n",
    "Instead of least-squares, weights can be optimised by minimising a loss function between the predicted- and observed target values, using [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). It is not an efficient method for this curve fitting problem, is only for the purpose of demonstrating how an iterative method can be implemented in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4030,
     "status": "ok",
     "timestamp": 1540321803016,
     "user": {
      "displayName": "Yipeng Hu",
      "photoUrl": "",
      "userId": "18139436242730223489"
     },
     "user_tz": -60
    },
    "id": "x2TDAsD-BKf_",
    "outputId": "01e04e4a-df38-41c5-9c56-db6a1109ed1d"
   },
   "outputs": [],
   "source": [
    "# build a new graph\n",
    "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
    "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
    "\n",
    "deg = 3\n",
    "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
    "\n",
    "# first declare variables that need optimisation\n",
    "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
    "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
    "\n",
    "# completing the computation graph with SGD\n",
    "node_1t = tf.matmul(node_X, var_w)\n",
    "# building a square loss\n",
    "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
    "# buiding a train-op to minimise the loss\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# launch a session\n",
    "sess = tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
    "\n",
    "# iteration to update variables with backprop gradients\n",
    "total_iter = int(1e4)\n",
    "indices_train = [i for i in range(n)]\n",
    "for step in range(total_iter):\n",
    "\n",
    "    idx = step % n\n",
    "    if idx == 0:  # shuffle every epoch\n",
    "        random.shuffle(indices_train)\n",
    "    \n",
    "    # single data point feed\n",
    "    singleDataFeed = {\n",
    "        ph_1x:x[indices_train[idx],np.newaxis], \n",
    "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
    "    \n",
    "    # update the variables\n",
    "    sess.run(train_op, feed_dict=singleDataFeed)\n",
    "    \n",
    "    # print training information\n",
    "    if (step % 200) == 0:\n",
    "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
    "        print('Step %d: Loss=%f' % (step, loss_train))\n",
    "    if (step % 2000) == 0:\n",
    "        w_sgd = sess.run(var_w)\n",
    "        print('Estimated weights:')\n",
    "        print(w_sgd)\n",
    "\n",
    "w_sgd = sess.run(var_w)\n",
    "print('Final weights at step %d:' % step)\n",
    "print(w_sgd)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxVSxLyeVdXb"
   },
   "source": [
    "## Questions\n",
    "- Try other optimisation hyperparameters, such as different optimiser, learning rate, number of iterations.\n",
    "- Try add regularisers and different loss functions.\n",
    "- Would batch gradient descent or minibatch gradient descent improve the optimisation?\n",
    "- Would higher-degree models more prone to overfitting?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorials_3-CurveFitting-TensorFlow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
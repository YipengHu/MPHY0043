{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDdbPOAglmA2"
   },
   "source": [
    "# 1 Linear Regression - Curve Fitting (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq93TYwjlmA8"
   },
   "source": [
    "## 1.1 Data generation\n",
    "Let the following polynomial be the \"true\" model. \n",
    "\n",
    "\\begin{equation}\n",
    "t = w_2x^2 + w_1x +w_0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "First we sample $n$ equidistant values for $x$, in a range of $[0,1]$; Then compute $n$ corresponding target values for $t$ using the above polynomial equation; To \"simulate\" the real-world scenario where the observed data usually come with noise, add random Gaussian noise to our target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNL18VKdlmBB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# get ground-truth data from the \"true\" model \n",
    "n = 20  # number of data samples\n",
    "x = [(idx-round(n/2))/(n/2) for idx in range(n)]\n",
    "print(x)\n",
    "\n",
    "w = [1, 2, 3]\n",
    "t = [xn**2*w[2] + xn*w[1] + w[0] for xn in x]\n",
    "print(t)\n",
    "\n",
    "# \"simulated\" observed target values\n",
    "std_noise = 0.2\n",
    "t_observed = [t[idx]+random.gauss(0,std_noise) for idx in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python plotting library [matplotlib](https://matplotlib.org/) is used to plot the observed target values *t_observed* with respect at each sampling location $x$ in blue points [$x$,*t_observed*]. We can also plot noise-free \"true\" target values $t$ versus the same $x$. This represents the underlying model as a curve, hence *curve fitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-vN0zD8lmBa"
   },
   "outputs": [],
   "source": [
    "# this line is useful for jupyter notebook only\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# plot the curve and the noise-corrupted data\n",
    "plt.plot(x,t,'r')\n",
    "plt.plot(x,t_observed,'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyAazZJKlmBx"
   },
   "source": [
    "## 1.2 Model Fitting\n",
    "For numerical computations such as curve fitting, we can use the powerful [numpy](http://www.numpy.org/). In this tutorial, we use [*numpy.linalg.lstsq*](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq) to solve [a system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) $\\textbf{a}\\textbf{x}=\\textbf{b}$ for a [least sqaures solution](https://en.wikipedia.org/wiki/Least_squares).\n",
    "\n",
    "Re-arrange the observed data to a linear system in matrix form \"$\\textbf{a}\\textbf{x}=\\textbf{b}$\" (N.B. the $\\textbf{x}$ is the unknown here, not the learning input $x$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{t} = \\textbf{X}\\textbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "that is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}\n",
    "t_{(1)} \\\\ t_{(2)} \\\\ \\cdots \\\\ t_{(n)}\n",
    "\\end{vmatrix} = \\begin{vmatrix}\n",
    "x^2_{(1)} & x_{(1)} & 1 \\\\\n",
    "x^2_{(2)} & x_{(2)} & 1 \\\\\n",
    "& \\cdots \\\\\n",
    "x^2_{(n)} & x_{(n)} & 1 \n",
    "\\end{vmatrix} \\cdot\n",
    "\\begin{vmatrix}\n",
    "w_2 \\\\ w_1 \\\\ w_0\n",
    "\\end{vmatrix}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLuMm_vlmB0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# then use nunmpy for a least-square solution to the linear system \"Xw=t\"\n",
    "t_observed = np.reshape(t_observed, [-1, 1])\n",
    "x_1 = np.reshape(x, [-1, 1])\n",
    "x_2 = np.square(x_1)\n",
    "x_0 = np.ones_like(x_1)\n",
    "X = np.concatenate([x_2,x_1,x_0],1)\n",
    "# print to check the inputs\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(t_observed.shape)\n",
    "w_estimate = np.linalg.lstsq(X, t_observed, rcond=None)\n",
    "print(w_estimate[0])  # print the output\n",
    "\n",
    "# plot to see the estimated curve, i.e.\n",
    "# t_estimate = [xn**2*w_estimate[2]+xn*w_estimate[1]+w_estimate[0] for xn in x]\n",
    "# but matrix multiplication is more compact:\n",
    "t_estimate = np.matmul(X,w_estimate[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*w_estimate* are the best estimates of the weights $w$ - in a least squares sense. Finally, we can add a green curve (formed by points [$x$, *t_estimate*]) to the previous plot to visualise the estimated polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLuMm_vlmB0"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,t,'r')\n",
    "plt.plot(x,t_observed,'bo')\n",
    "plt.plot(x,t_estimate,'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zaoUWBtlmCC"
   },
   "source": [
    "## 1.3 Model Fitting Error\n",
    "From the above plot, we can see that the green curve (i.e. the fitted polynomial model) does not go through all the observed points (in blue) which were used in fitting the model. This is known as redidual error, also known as training error. There is also discrepancy between the \"true\" curve (in red) and the estimated one (in green), but this difference is often impossible to obtain without access to the true underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxRPpS-s_EPP"
   },
   "outputs": [],
   "source": [
    "# residuals:\n",
    "Residuals = t_estimate-t_observed\n",
    "SR = np.sum(np.square(Residuals))  # sums of residuals: b - a*x\n",
    "# root-mean-square error\n",
    "RMSE = np.sqrt(np.mean(np.square(Residuals)))\n",
    "print(SR)\n",
    "print(RMSE)\n",
    "\n",
    "# plot the error distribution\n",
    "plt.hist(Residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUlz_pjSSwv9"
   },
   "source": [
    "## Questions\n",
    "### Sample size\n",
    "- Effect on changing the sample size on model fitting and its errors.\n",
    "- How many samples are needed? \n",
    "\n",
    "### Model fitting\n",
    "- What is the objective (loss) function in this curve fitting problem?\n",
    "- How does change of the noise level (i.e. *std_noise*) affect the model fitting?\n",
    "- Are residualds a good estimate of the difference to the \"true\" target values?\n",
    "- How to measure how good the estimates are?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorials_01-LinearRegression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
